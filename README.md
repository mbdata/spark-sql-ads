## Big Data Analytics Tools
## Project: Apache Spark SQL Project

*by **Michael Baranov***

# Introduction
This project had the following objectives:

1. To use Apache Spark as an analytic tool to analyze realistic data.
2. To give experience working with open-ended problems, that are similar to problems that one will face in career as data professional.
3. At the end of this project one should:
  a. Gain sufficient confidence in using Apache Spark.
  b. Gain an appetite for working with large datasets.
  c. Be aware of the potential and benefits of analyzing large datasets.
4. Demonstrate a good understanding of the features that Apache Spark provides.
5. Demonstrate ability to work with multiple large datasets and use the tools to gain valuable insights from the datasets. 
6. Be able to present these insights in a manner that is easily consumable by interested parties. 

# Environment Preparation

A Linux - based [Cloud Spark environment provided by Databricks](https://databricks.com/try-databricks) was chosen for this project. Databricks Unified Analytics Platform, from the original creators of Apache Sparkâ„¢, is a unifies data science and engineering across the Machine Learning lifecycle from data preparation, to experimentation and deployment of ML applications. It is a fully managed service and reduce infrastructure complexity to focus more on innovation, while keeping data safe and secure. The Community edition was chosen for this project since it is free and provides the following environment:

- Single cluster limited to 6GB and no worker nodes.
- Basic notebook without collaboration.
- Limited to 3 max users.
- Public environment to share work.
 
The dataset analysis was done in Databrisks Spark SQL notebook which is Databricks' hosted implementation of Apache Zeppelin environment.
